{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48c660a-07d2-48ef-8638-7f979bada293",
   "metadata": {},
   "source": [
    "Q1. Random Forest Regressor is an ensemble learning method used for regression tasks. It is based on the Random Forest algorithm, which combines the predictions of multiple decision trees to improve the accuracy and robustness of the model.\n",
    "\n",
    "Q2. Random Forest Regressor reduces the risk of overfitting by training multiple decision trees on different subsets of the training data. Each tree is trained independently, and then their predictions are aggregated through averaging (for regression tasks) or voting (for classification tasks). By introducing randomness in the feature selection and bootstrapping process, Random Forest reduces the variance of the model, which helps in generalizing better to unseen data and mitigating overfitting.\n",
    "\n",
    "Q3. Random Forest Regressor aggregates the predictions of multiple decision trees by taking the average of their individual predictions. For regression tasks, the final prediction is the mean prediction of all the trees in the forest.\n",
    "\n",
    "Q4. Some common hyperparameters of Random Forest Regressor include:\n",
    "\n",
    "n_estimators: Number of decision trees in the forest.\n",
    "max_depth: Maximum depth of each decision tree.\n",
    "min_samples_split: Minimum number of samples required to split an internal node.\n",
    "min_samples_leaf: Minimum number of samples required to be at a leaf node.\n",
    "max_features: Number of features to consider when looking for the best split.\n",
    "Q5. The main difference between Random Forest Regressor and Decision Tree Regressor lies in the way predictions are aggregated. While Decision Tree Regressor uses a single decision tree to make predictions, Random Forest Regressor utilizes an ensemble of multiple decision trees and aggregates their predictions to improve accuracy and robustness.\n",
    "\n",
    "Q6. Advantages of Random Forest Regressor:\n",
    "\n",
    "Handles large datasets with high dimensionality effectively.\n",
    "Less prone to overfitting compared to individual decision trees.\n",
    "Automatically handles missing values and maintains accuracy with noisy data.\n",
    "Provides estimates of feature importance.\n",
    "Disadvantages:\n",
    "Less interpretable compared to individual decision trees.\n",
    "Can be computationally expensive, especially with a large number of trees and features.\n",
    "May not perform well on extrapolation tasks.\n",
    "Q7. The output of Random Forest Regressor is a continuous value representing the predicted response variable (e.g., house price, stock price) for each input sample.\n",
    "\n",
    "Q8. While Random Forest Regressor is primarily designed for regression tasks, Random Forest Classifier is commonly used for classification tasks. The underlying mechanism and principles are similar, but Random Forest Classifier predicts class labels instead of continuous values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
